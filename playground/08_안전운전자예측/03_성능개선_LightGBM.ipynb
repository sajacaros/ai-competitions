{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 08 안전 운전자 예측\n",
    "## 8.4 성능 개선 I: LightGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lightgbm import early_stopping, log_evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = '../../data/08_safe_driver/'\n",
    "\n",
    "train_df = pd.read_csv(data_path+'train.csv', index_col='id')\n",
    "test_df = pd.read_csv(data_path+'test.csv', index_col='id')\n",
    "submission_df = pd.read_csv(data_path+'sample_submission.csv', index_col='id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.4.1 피처 엔지니어링"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 데이터 합치기\n",
    "all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "all_data = all_data.drop(columns='target')\n",
    "all_features = all_data.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 명목형 피처 원핫 인코딩\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 파생 피처 추가\n",
    "## 결측값 개수를 파생 피처로 생성\n",
    "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 필요 없는 피처 제거\n",
    "# drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "remaining_features = [feature for feature in all_features if ('cat' not in feature and 'calc' not in feature)] # and feature not in drop_features)]\n",
    "remaining_features.append('num_missing')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## ind 분류의 피처 처리\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
    "        is_first_feature=False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0    2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n1     1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n2    5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n3     0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n4     0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n5     5_1_4_0_0_0_0_0_1_0_0_0_0_0_6_1_0_0_\n6     2_1_3_1_0_0_1_0_0_0_0_0_0_0_8_1_0_0_\n7    5_1_4_0_0_1_0_0_0_0_0_0_0_0_13_1_0_0_\n8     5_1_3_1_0_0_0_1_0_0_0_0_0_0_6_1_0_0_\n9     1_1_2_0_0_0_1_0_0_0_0_0_0_0_4_0_0_1_\nName: mix_ind, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(10)['mix_ind']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "mix_ind\n0_2_1_0_0_1_0_0_0_0_0_0_0_0_7_1_0_0_     2992\n0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_     2784\n0_1_2_0_0_1_0_0_0_0_0_0_0_0_7_1_0_0_     2568\n0_1_1_0_0_1_0_0_0_0_0_0_0_0_7_1_0_0_     2174\n0_2_0_0_0_1_0_0_0_0_0_0_0_0_7_1_0_0_     2131\n                                         ... \n0_2_0_0_6_0_0_0_1_0_0_0_0_0_4_0_1_0_        1\n6_4_7_0_0_0_0_1_0_0_0_0_0_0_7_1_0_0_        1\n3_4_8_1_2_0_0_0_1_0_0_0_0_0_12_1_0_0_       1\n5_4_6_1_2_0_1_0_0_0_0_0_0_0_4_1_0_0_        1\n5_1_8_0_0_0_1_0_0_0_1_0_0_1_3_0_0_1_        1\nName: count, Length: 143769, dtype: int64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "ps_ind_02_cat\n 1    1079327\n 2     309747\n 3      70172\n 4      28259\n-1        523\nName: count, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 명목형 피처의 고유값별 개수를 새로운 피처로 추가\n",
    "all_data['ps_ind_02_cat'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "    cat_count_features.append(f'{feature}_count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 지금까지 만든 피처\n",
    "* encoded_cat_matrix\n",
    "    * 원핫 인코딩된 명목형 피처\n",
    "* remaining_features\n",
    "    * 명목형 피처와 calc 분류의 피처를 제외한 피처들 + num_missing\n",
    "* cat_count_features\n",
    "    * mix_ind를 포함한 명목형 피처의 고유값별 개수 파생 피처\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n0          2              2          5              1              0   \n1          1              1          7              0              0   \n2          5              4          9              1              0   \n3          0              1          2              0              0   \n4          0              2          0              1              0   \n\n   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n0              0              1              0              0              0   \n1              0              0              1              0              0   \n2              0              0              1              0              0   \n3              1              0              0              0              0   \n4              1              0              0              0              0   \n\n   ...  ps_car_03_cat_count  ps_car_04_cat_count  ps_car_05_cat_count  \\\n0  ...              1028142              1241334               431560   \n1  ...              1028142              1241334               666910   \n2  ...              1028142              1241334               666910   \n3  ...               183044              1241334               431560   \n4  ...              1028142              1241334               666910   \n\n   ps_car_06_cat_count  ps_car_07_cat_count  ps_car_08_cat_count  \\\n0                77845              1383070               249663   \n1               329890              1383070              1238365   \n2               147714              1383070              1238365   \n3               329890              1383070              1238365   \n4               147714              1383070              1238365   \n\n   ps_car_09_cat_count  ps_car_10_cat_count  ps_car_11_cat_count  \\\n0               486510              1475460                18326   \n1               883326              1475460                12535   \n2               883326              1475460                19943   \n3                36798              1475460               212989   \n4               883326              1475460                26161   \n\n   mix_ind_count  \n0              6  \n1             36  \n2             24  \n3           2784  \n4            258  \n\n[5 rows x 74 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ps_ind_01</th>\n      <th>ps_ind_02_cat</th>\n      <th>ps_ind_03</th>\n      <th>ps_ind_04_cat</th>\n      <th>ps_ind_05_cat</th>\n      <th>ps_ind_06_bin</th>\n      <th>ps_ind_07_bin</th>\n      <th>ps_ind_08_bin</th>\n      <th>ps_ind_09_bin</th>\n      <th>ps_ind_10_bin</th>\n      <th>...</th>\n      <th>ps_car_03_cat_count</th>\n      <th>ps_car_04_cat_count</th>\n      <th>ps_car_05_cat_count</th>\n      <th>ps_car_06_cat_count</th>\n      <th>ps_car_07_cat_count</th>\n      <th>ps_car_08_cat_count</th>\n      <th>ps_car_09_cat_count</th>\n      <th>ps_car_10_cat_count</th>\n      <th>ps_car_11_cat_count</th>\n      <th>mix_ind_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>77845</td>\n      <td>1383070</td>\n      <td>249663</td>\n      <td>486510</td>\n      <td>1475460</td>\n      <td>18326</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>12535</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>147714</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>19943</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>183044</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>36798</td>\n      <td>1475460</td>\n      <td>212989</td>\n      <td>2784</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>147714</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>26161</td>\n      <td>258</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 74 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "   ps_ind_01  ps_ind_03  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  \\\n0          2          5              0              1              0   \n1          1          7              0              0              1   \n2          5          9              0              0              1   \n3          0          2              1              0              0   \n4          0          0              1              0              0   \n\n   ps_ind_09_bin  ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ...  \\\n0              0         11              0              1              0  ...   \n1              0          3              0              0              1  ...   \n2              0         12              1              0              0  ...   \n3              0          8              1              0              0  ...   \n4              0          9              1              0              0  ...   \n\n   ps_car_03_cat_count  ps_car_04_cat_count  ps_car_05_cat_count  \\\n0              1028142              1241334               431560   \n1              1028142              1241334               666910   \n2              1028142              1241334               666910   \n3               183044              1241334               431560   \n4              1028142              1241334               666910   \n\n   ps_car_06_cat_count  ps_car_07_cat_count  ps_car_08_cat_count  \\\n0                77845              1383070               249663   \n1               329890              1383070              1238365   \n2               147714              1383070              1238365   \n3               329890              1383070              1238365   \n4               147714              1383070              1238365   \n\n   ps_car_09_cat_count  ps_car_10_cat_count  ps_car_11_cat_count  \\\n0               486510              1475460                18326   \n1               883326              1475460                12535   \n2               883326              1475460                19943   \n3                36798              1475460               212989   \n4               883326              1475460                26161   \n\n   mix_ind_count  \n0              6  \n1             36  \n2             24  \n3           2784  \n4            258  \n\n[5 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ps_ind_01</th>\n      <th>ps_ind_03</th>\n      <th>ps_ind_06_bin</th>\n      <th>ps_ind_07_bin</th>\n      <th>ps_ind_08_bin</th>\n      <th>ps_ind_09_bin</th>\n      <th>ps_ind_15</th>\n      <th>ps_ind_16_bin</th>\n      <th>ps_ind_17_bin</th>\n      <th>ps_ind_18_bin</th>\n      <th>...</th>\n      <th>ps_car_03_cat_count</th>\n      <th>ps_car_04_cat_count</th>\n      <th>ps_car_05_cat_count</th>\n      <th>ps_car_06_cat_count</th>\n      <th>ps_car_07_cat_count</th>\n      <th>ps_car_08_cat_count</th>\n      <th>ps_car_09_cat_count</th>\n      <th>ps_car_10_cat_count</th>\n      <th>ps_car_11_cat_count</th>\n      <th>mix_ind_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>77845</td>\n      <td>1383070</td>\n      <td>249663</td>\n      <td>486510</td>\n      <td>1475460</td>\n      <td>18326</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>12535</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>147714</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>19943</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>183044</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>36798</td>\n      <td>1475460</td>\n      <td>212989</td>\n      <td>2784</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>147714</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>26161</td>\n      <td>258</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 33 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요없는 피처 제거\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(columns=drop_features)\n",
    "all_data_remaining.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "all_data_sprs = sparse.hstack([\n",
    "    sparse.csr_matrix(all_data_remaining),\n",
    "    encoded_cat_matrix\n",
    "], format='csr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 데이터 나누기\n",
    "num_train = len(train_df)\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "y = train_df['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def eval_gini(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    L_mid = np.linspace(1/n_samples, 1, n_samples) # 대각선 값\n",
    "\n",
    "    # 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()]\n",
    "    # 로렌츠 곡선\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # array / scalar = array\n",
    "    G_pred = np.sum(L_mid - L_pred) # 예측값에 대한 지니계수\n",
    "\n",
    "    # 예측이 완벽할 때 지니계수\n",
    "    true_order = y_true[y_true.argsort()]\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true) # 예측이 완벽할 때의 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.4.2 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 데이터셋 준비\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파리미터 범위 설정\n",
    "* 하이퍼파라미터 범위를 점점 좁히는 방법\n",
    "* 다른 상위권 캐글러가 설정한 하이퍼파라미터룰 참고하는 방법"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "param_bounds = {\n",
    "    'num_leaves': (30,40),\n",
    "    'lambda_l1': (0.7, 0.9),\n",
    "    'lambda_l2': (0.9, 1),\n",
    "    'feature_fraction': (0.6, 0.7),\n",
    "    'bagging_fraction': (0.6, 0.9),\n",
    "    'min_child_samples': (6, 10),\n",
    "    'min_child_weight': (10, 40)\n",
    "}\n",
    "\n",
    "fixed_params = {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'random_state': 1991\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# LightGBM용 gini() 함수\n",
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label() # 데이터셋의 타깃값을 반환\n",
    "    #    평가지표 이름       평가 점수     평가 점수가 높을 수록 좋은지 여부\n",
    "    return 'gini',   eval_gini(labels, preds),      True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 베이지안 최적화용 평가지표 계산 함수 작성\n",
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction, bagging_fraction, min_child_samples, min_child_weight):\n",
    "    params = {\n",
    "        'num_leaves': int(round(num_leaves)),\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'feature_pre_filter': False\n",
    "    }\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print(f'하이퍼파라미터: {params}')\n",
    "\n",
    "    lgb_model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=bayes_dtrain,\n",
    "        num_boost_round=2500,\n",
    "        valid_sets=bayes_dvalid,\n",
    "        feval=gini,\n",
    "        callbacks=[early_stopping(300)] #, log_evaluation(100)],\n",
    "        # early_stopping_rounds=300,\n",
    "        # verbose_eval=False\n",
    "    )\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 최적화 수행\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=eval_function,\n",
    "    pbounds=param_bounds,\n",
    "    random_state=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.8675   \u001B[0m | \u001B[0m0.6964   \u001B[0m | \u001B[0m0.7767   \u001B[0m | \u001B[0m0.9792   \u001B[0m | \u001B[0m8.116    \u001B[0m | \u001B[0m27.04    \u001B[0m | \u001B[0m39.26    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001B[95m2        \u001B[0m | \u001B[95m0.2858   \u001B[0m | \u001B[95m0.6213   \u001B[0m | \u001B[95m0.6087   \u001B[0m | \u001B[95m0.704    \u001B[0m | \u001B[95m0.9833   \u001B[0m | \u001B[95m9.113    \u001B[0m | \u001B[95m36.1     \u001B[0m | \u001B[95m39.79    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.8561058352572911, 'lambda_l2': 0.9118274425868933, 'feature_fraction': 0.6461479362252931, 'bagging_fraction': 0.8397475692650171, 'min_child_samples': 9, 'min_child_weight': 14.300598622271393, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2843646083962795\n",
      "\n",
      "| \u001B[0m3        \u001B[0m | \u001B[0m0.2844   \u001B[0m | \u001B[0m0.8397   \u001B[0m | \u001B[0m0.6461   \u001B[0m | \u001B[0m0.8561   \u001B[0m | \u001B[0m0.9118   \u001B[0m | \u001B[0m8.56     \u001B[0m | \u001B[0m14.3     \u001B[0m | \u001B[0m39.45    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.7529111224209254, 'lambda_l2': 0.9774233689434216, 'feature_fraction': 0.6414661939990524, 'bagging_fraction': 0.7565544965250215, 'min_child_samples': 8, 'min_child_weight': 27.053018466059456, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.285133186199933\n",
      "\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m0.2851   \u001B[0m | \u001B[0m0.7566   \u001B[0m | \u001B[0m0.6415   \u001B[0m | \u001B[0m0.7529   \u001B[0m | \u001B[0m0.9774   \u001B[0m | \u001B[0m7.825    \u001B[0m | \u001B[0m27.05    \u001B[0m | \u001B[0m30.19    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8233867993749514, 'lambda_l2': 0.9943748078514624, 'feature_fraction': 0.6612095722722421, 'bagging_fraction': 0.7852906491227631, 'min_child_samples': 9, 'min_child_weight': 20.785237017213582, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28413103026141523\n",
      "\n",
      "| \u001B[0m5        \u001B[0m | \u001B[0m0.2841   \u001B[0m | \u001B[0m0.7853   \u001B[0m | \u001B[0m0.6612   \u001B[0m | \u001B[0m0.8234   \u001B[0m | \u001B[0m0.9944   \u001B[0m | \u001B[0m8.727    \u001B[0m | \u001B[0m20.79    \u001B[0m | \u001B[0m34.37    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m0.2829   \u001B[0m | \u001B[0m0.8978   \u001B[0m | \u001B[0m0.6594   \u001B[0m | \u001B[0m0.8445   \u001B[0m | \u001B[0m0.9234   \u001B[0m | \u001B[0m8.619    \u001B[0m | \u001B[0m10.55    \u001B[0m | \u001B[0m30.09    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001B[0m7        \u001B[0m | \u001B[0m0.2851   \u001B[0m | \u001B[0m0.7667   \u001B[0m | \u001B[0m0.6606   \u001B[0m | \u001B[0m0.7738   \u001B[0m | \u001B[0m0.9033   \u001B[0m | \u001B[0m8.769    \u001B[0m | \u001B[0m29.31    \u001B[0m | \u001B[0m36.6     \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7113567244294035, 'lambda_l2': 0.9992148463611682, 'feature_fraction': 0.6823972673568225, 'bagging_fraction': 0.6452323984860321, 'min_child_samples': 9, 'min_child_weight': 36.23198396337493, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28549714593923864\n",
      "\n",
      "| \u001B[0m8        \u001B[0m | \u001B[0m0.2855   \u001B[0m | \u001B[0m0.6452   \u001B[0m | \u001B[0m0.6824   \u001B[0m | \u001B[0m0.7114   \u001B[0m | \u001B[0m0.9992   \u001B[0m | \u001B[0m9.083    \u001B[0m | \u001B[0m36.23    \u001B[0m | \u001B[0m39.59    \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6374163087819378, 'min_child_samples': 9, 'min_child_weight': 35.07917770932417, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2858822213198542\n",
      "\n",
      "| \u001B[95m9        \u001B[0m | \u001B[95m0.2859   \u001B[0m | \u001B[95m0.6374   \u001B[0m | \u001B[95m0.6      \u001B[0m | \u001B[95m0.7      \u001B[0m | \u001B[95m0.9      \u001B[0m | \u001B[95m9.496    \u001B[0m | \u001B[95m35.08    \u001B[0m | \u001B[95m40.0     \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 8, 'min_child_weight': 35.037970582965684, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2848427308895299\n",
      "\n",
      "| \u001B[0m10       \u001B[0m | \u001B[0m0.2848   \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m7.843    \u001B[0m | \u001B[0m35.04    \u001B[0m | \u001B[0m40.0     \u001B[0m |\n",
      "하이퍼파라미터: {'num_leaves': 38, 'lambda_l1': 0.8397572814771797, 'lambda_l2': 0.9008140279489394, 'feature_fraction': 0.62583667466163, 'bagging_fraction': 0.8751015355558421, 'min_child_samples': 10, 'min_child_weight': 34.5260863487709, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28435840552500996\n",
      "\n",
      "| \u001B[0m11       \u001B[0m | \u001B[0m0.2844   \u001B[0m | \u001B[0m0.8751   \u001B[0m | \u001B[0m0.6258   \u001B[0m | \u001B[0m0.8398   \u001B[0m | \u001B[0m0.9008   \u001B[0m | \u001B[0m9.824    \u001B[0m | \u001B[0m34.53    \u001B[0m | \u001B[0m38.42    \u001B[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=3, n_iter=6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.6374163087819378,\n 'feature_fraction': 0.6,\n 'lambda_l1': 0.7,\n 'lambda_l2': 0.9,\n 'min_child_samples': 9.4959504898169,\n 'min_child_weight': 35.07917770932417,\n 'num_leaves': 40.0}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# 값이 고정된 파라미터 추가\n",
    "max_params.update(fixed_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.6374163087819378,\n 'feature_fraction': 0.6,\n 'lambda_l1': 0.7,\n 'lambda_l2': 0.9,\n 'min_child_samples': 9,\n 'min_child_weight': 35.07917770932417,\n 'num_leaves': 40,\n 'objective': 'binary',\n 'learning_rate': 0.005,\n 'bagging_freq': 1,\n 'force_row_wise': True,\n 'random_state': 1991}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.4.3 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# OOF 방식으로 LightGBM 훈련\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\dukim\\miniconda3\\envs\\musthave_mldl_problem_solving_strategy\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154243\tvalid_0's gini: 0.271218\n",
      "[200]\tvalid_0's binary_logloss: 0.153179\tvalid_0's gini: 0.276022\n",
      "[300]\tvalid_0's binary_logloss: 0.152584\tvalid_0's gini: 0.279766\n",
      "[400]\tvalid_0's binary_logloss: 0.152219\tvalid_0's gini: 0.283476\n",
      "[500]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.286633\n",
      "[600]\tvalid_0's binary_logloss: 0.151829\tvalid_0's gini: 0.289077\n",
      "[700]\tvalid_0's binary_logloss: 0.151717\tvalid_0's gini: 0.290887\n",
      "[800]\tvalid_0's binary_logloss: 0.15163\tvalid_0's gini: 0.292622\n",
      "[900]\tvalid_0's binary_logloss: 0.151566\tvalid_0's gini: 0.293937\n",
      "[1000]\tvalid_0's binary_logloss: 0.151519\tvalid_0's gini: 0.295018\n",
      "[1100]\tvalid_0's binary_logloss: 0.151481\tvalid_0's gini: 0.295785\n",
      "[1200]\tvalid_0's binary_logloss: 0.151446\tvalid_0's gini: 0.29675\n",
      "[1300]\tvalid_0's binary_logloss: 0.15142\tvalid_0's gini: 0.297367\n",
      "[1400]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.297874\n",
      "[1500]\tvalid_0's binary_logloss: 0.151384\tvalid_0's gini: 0.298318\n",
      "[1600]\tvalid_0's binary_logloss: 0.151375\tvalid_0's gini: 0.29844\n",
      "[1700]\tvalid_0's binary_logloss: 0.151369\tvalid_0's gini: 0.298578\n",
      "[1800]\tvalid_0's binary_logloss: 0.151364\tvalid_0's gini: 0.298673\n",
      "[1900]\tvalid_0's binary_logloss: 0.15136\tvalid_0's gini: 0.298741\n",
      "[2000]\tvalid_0's binary_logloss: 0.15136\tvalid_0's gini: 0.298705\n",
      "[2100]\tvalid_0's binary_logloss: 0.151357\tvalid_0's gini: 0.298845\n",
      "[2200]\tvalid_0's binary_logloss: 0.15136\tvalid_0's gini: 0.298712\n",
      "[2300]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.298643\n",
      "Early stopping, best iteration is:\n",
      "[2078]\tvalid_0's binary_logloss: 0.151356\tvalid_0's gini: 0.298874\n",
      "폴드 1 지니계수 : 0.298874261479776\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154341\tvalid_0's gini: 0.259597\n",
      "[200]\tvalid_0's binary_logloss: 0.153324\tvalid_0's gini: 0.264714\n",
      "[300]\tvalid_0's binary_logloss: 0.152787\tvalid_0's gini: 0.268516\n",
      "[400]\tvalid_0's binary_logloss: 0.15247\tvalid_0's gini: 0.2715\n",
      "[500]\tvalid_0's binary_logloss: 0.152278\tvalid_0's gini: 0.274045\n",
      "[600]\tvalid_0's binary_logloss: 0.152142\tvalid_0's gini: 0.276219\n",
      "[700]\tvalid_0's binary_logloss: 0.152054\tvalid_0's gini: 0.277691\n",
      "[800]\tvalid_0's binary_logloss: 0.151978\tvalid_0's gini: 0.27945\n",
      "[900]\tvalid_0's binary_logloss: 0.151927\tvalid_0's gini: 0.280602\n",
      "[1000]\tvalid_0's binary_logloss: 0.151878\tvalid_0's gini: 0.281909\n",
      "[1100]\tvalid_0's binary_logloss: 0.151844\tvalid_0's gini: 0.282756\n",
      "[1200]\tvalid_0's binary_logloss: 0.151817\tvalid_0's gini: 0.283426\n",
      "[1300]\tvalid_0's binary_logloss: 0.151796\tvalid_0's gini: 0.283946\n",
      "[1400]\tvalid_0's binary_logloss: 0.15178\tvalid_0's gini: 0.284398\n",
      "[1500]\tvalid_0's binary_logloss: 0.151775\tvalid_0's gini: 0.28463\n",
      "[1600]\tvalid_0's binary_logloss: 0.151765\tvalid_0's gini: 0.28488\n",
      "[1700]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.285019\n",
      "[1800]\tvalid_0's binary_logloss: 0.151742\tvalid_0's gini: 0.285501\n",
      "[1900]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.285615\n",
      "[2000]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.285606\n",
      "[2100]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.285797\n",
      "[2200]\tvalid_0's binary_logloss: 0.151731\tvalid_0's gini: 0.285802\n",
      "[2300]\tvalid_0's binary_logloss: 0.151728\tvalid_0's gini: 0.285909\n",
      "[2400]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.286015\n",
      "[2500]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.285836\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2427]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.286016\n",
      "폴드 2 지니계수 : 0.2860158456371415\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154247\tvalid_0's gini: 0.263377\n",
      "[200]\tvalid_0's binary_logloss: 0.153176\tvalid_0's gini: 0.268462\n",
      "[300]\tvalid_0's binary_logloss: 0.152592\tvalid_0's gini: 0.271943\n",
      "[400]\tvalid_0's binary_logloss: 0.152245\tvalid_0's gini: 0.274826\n",
      "[500]\tvalid_0's binary_logloss: 0.152029\tvalid_0's gini: 0.277068\n",
      "[600]\tvalid_0's binary_logloss: 0.151876\tvalid_0's gini: 0.279221\n",
      "[700]\tvalid_0's binary_logloss: 0.151776\tvalid_0's gini: 0.280818\n",
      "[800]\tvalid_0's binary_logloss: 0.151707\tvalid_0's gini: 0.281934\n",
      "[900]\tvalid_0's binary_logloss: 0.151657\tvalid_0's gini: 0.282864\n",
      "[1000]\tvalid_0's binary_logloss: 0.15162\tvalid_0's gini: 0.283644\n",
      "[1100]\tvalid_0's binary_logloss: 0.151604\tvalid_0's gini: 0.283865\n",
      "[1200]\tvalid_0's binary_logloss: 0.151582\tvalid_0's gini: 0.28438\n",
      "[1300]\tvalid_0's binary_logloss: 0.151573\tvalid_0's gini: 0.284595\n",
      "[1400]\tvalid_0's binary_logloss: 0.151571\tvalid_0's gini: 0.284509\n",
      "[1500]\tvalid_0's binary_logloss: 0.151566\tvalid_0's gini: 0.284647\n",
      "[1600]\tvalid_0's binary_logloss: 0.151571\tvalid_0's gini: 0.284456\n",
      "[1700]\tvalid_0's binary_logloss: 0.15157\tvalid_0's gini: 0.284489\n",
      "Early stopping, best iteration is:\n",
      "[1492]\tvalid_0's binary_logloss: 0.151565\tvalid_0's gini: 0.28469\n",
      "폴드 3 지니계수 : 0.28468996648844314\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154325\tvalid_0's gini: 0.257655\n",
      "[200]\tvalid_0's binary_logloss: 0.153302\tvalid_0's gini: 0.262666\n",
      "[300]\tvalid_0's binary_logloss: 0.152753\tvalid_0's gini: 0.265932\n",
      "[400]\tvalid_0's binary_logloss: 0.152436\tvalid_0's gini: 0.26875\n",
      "[500]\tvalid_0's binary_logloss: 0.152245\tvalid_0's gini: 0.271069\n",
      "[600]\tvalid_0's binary_logloss: 0.152115\tvalid_0's gini: 0.27298\n",
      "[700]\tvalid_0's binary_logloss: 0.152027\tvalid_0's gini: 0.274528\n",
      "[800]\tvalid_0's binary_logloss: 0.151961\tvalid_0's gini: 0.275889\n",
      "[900]\tvalid_0's binary_logloss: 0.151913\tvalid_0's gini: 0.277027\n",
      "[1000]\tvalid_0's binary_logloss: 0.15188\tvalid_0's gini: 0.277838\n",
      "[1100]\tvalid_0's binary_logloss: 0.151858\tvalid_0's gini: 0.278352\n",
      "[1200]\tvalid_0's binary_logloss: 0.151841\tvalid_0's gini: 0.278877\n",
      "[1300]\tvalid_0's binary_logloss: 0.151827\tvalid_0's gini: 0.279268\n",
      "[1400]\tvalid_0's binary_logloss: 0.151813\tvalid_0's gini: 0.279766\n",
      "[1500]\tvalid_0's binary_logloss: 0.151809\tvalid_0's gini: 0.279982\n",
      "[1600]\tvalid_0's binary_logloss: 0.151803\tvalid_0's gini: 0.280146\n",
      "[1700]\tvalid_0's binary_logloss: 0.151808\tvalid_0's gini: 0.280072\n",
      "[1800]\tvalid_0's binary_logloss: 0.151805\tvalid_0's gini: 0.280305\n",
      "[1900]\tvalid_0's binary_logloss: 0.151811\tvalid_0's gini: 0.280218\n",
      "Early stopping, best iteration is:\n",
      "[1641]\tvalid_0's binary_logloss: 0.151802\tvalid_0's gini: 0.280206\n",
      "폴드 4 지니계수 : 0.2802056560125199\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154396\tvalid_0's gini: 0.266176\n",
      "[200]\tvalid_0's binary_logloss: 0.15339\tvalid_0's gini: 0.271527\n",
      "[300]\tvalid_0's binary_logloss: 0.152831\tvalid_0's gini: 0.275332\n",
      "[400]\tvalid_0's binary_logloss: 0.152505\tvalid_0's gini: 0.278568\n",
      "[500]\tvalid_0's binary_logloss: 0.152282\tvalid_0's gini: 0.281978\n",
      "[600]\tvalid_0's binary_logloss: 0.152135\tvalid_0's gini: 0.284654\n",
      "[700]\tvalid_0's binary_logloss: 0.152028\tvalid_0's gini: 0.286753\n",
      "[800]\tvalid_0's binary_logloss: 0.151943\tvalid_0's gini: 0.288733\n",
      "[900]\tvalid_0's binary_logloss: 0.151882\tvalid_0's gini: 0.290262\n",
      "[1000]\tvalid_0's binary_logloss: 0.151833\tvalid_0's gini: 0.291493\n",
      "[1100]\tvalid_0's binary_logloss: 0.151795\tvalid_0's gini: 0.292443\n",
      "[1200]\tvalid_0's binary_logloss: 0.151765\tvalid_0's gini: 0.293249\n",
      "[1300]\tvalid_0's binary_logloss: 0.151743\tvalid_0's gini: 0.293793\n",
      "[1400]\tvalid_0's binary_logloss: 0.151733\tvalid_0's gini: 0.29403\n",
      "[1500]\tvalid_0's binary_logloss: 0.151723\tvalid_0's gini: 0.294355\n",
      "[1600]\tvalid_0's binary_logloss: 0.151706\tvalid_0's gini: 0.294808\n",
      "[1700]\tvalid_0's binary_logloss: 0.151695\tvalid_0's gini: 0.295154\n",
      "[1800]\tvalid_0's binary_logloss: 0.151687\tvalid_0's gini: 0.29537\n",
      "[1900]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.295487\n",
      "[2000]\tvalid_0's binary_logloss: 0.151685\tvalid_0's gini: 0.295486\n",
      "[2100]\tvalid_0's binary_logloss: 0.151685\tvalid_0's gini: 0.295433\n",
      "[2200]\tvalid_0's binary_logloss: 0.151685\tvalid_0's gini: 0.295394\n",
      "Early stopping, best iteration is:\n",
      "[1927]\tvalid_0's binary_logloss: 0.15168\tvalid_0's gini: 0.295631\n",
      "폴드 5 지니계수 : 0.29563130763533463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "    lgb_model = lgb.train(\n",
    "        params=max_params,             # 훈련용 하이퍼파라미터\n",
    "        train_set=dtrain,          # 훈련 데이터셋\n",
    "        num_boost_round=2500,      # 부스팅 반복 횟수\n",
    "        valid_sets=dvalid,         # 상능 평가용 검증 데이터셋\n",
    "        feval=gini,                # 검증용 평가지표\n",
    "        callbacks=[early_stopping(300), log_evaluation(100)],# 조기종료 조건, 100번째마다 점수 출력\n",
    "        # early_stopping_rounds=300,\n",
    "        # verbose_eval=100\n",
    "    )\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수: 0.2889965811339316\n"
     ]
    }
   ],
   "source": [
    "print(f'OOF 검증 데이터 지니계수: {eval_gini(y, oof_val_preds)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "submission_df['target'] = oof_test_preds\n",
    "submission_df.to_csv(data_path+'submission_enhanced1.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
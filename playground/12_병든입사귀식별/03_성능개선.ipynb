{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12.4 성능 개선\n",
    "* 에폭 늘리기\n",
    "* 스케줄러 추가\n",
    "* TTA(테스트 단계 데이터 증강) 기법\n",
    "* 레이블 스무딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from timm import create_model\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 50\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# 장비 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_path = '../../data/12_plant/'\n",
    "train = pd.read_csv(data_path + 'train.csv')\n",
    "test = pd.read_csv(data_path + 'test.csv')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv')\n",
    "\n",
    "train, valid = train_test_split(\n",
    "    train,\n",
    "    test_size=0.1,\n",
    "    stratify=train[['healthy', 'multiple_diseases', 'rust', 'scab']],\n",
    "    random_state=50\n",
    ")\n",
    "\n",
    "class ImageDataSet(Dataset):\n",
    "    def __init__(self, df, img_dir=data_path+'images/', transform=None, is_test=False):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform if transform else lambda x:x\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx, 0]\n",
    "        img_path = self.img_dir + img_id + '.jpg'\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image=image)['image']\n",
    "        if self.is_test:\n",
    "            return image\n",
    "        else:\n",
    "            label = np.argmax(self.df.iloc[idx, 1:5])\n",
    "            return image, label\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    A.Resize(450, 650), # 이미지 크기 조절\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3), # 밝기 대비 조절\n",
    "    A.VerticalFlip(p=0.2), # 상하 대칭 변환\n",
    "    A.HorizontalFlip(p=0.5), # 좌우 대칭 변환\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.3), # 이동, 스케일링, 회전\n",
    "    A.OneOf([\n",
    "        A.Emboss(p=1), # 양각화\n",
    "        A.Sharpen(p=1), # 날카로움\n",
    "        A.Blur(p=1) # 불러 효과\n",
    "    ], p=0.3),\n",
    "    A.PiecewiseAffine(p=0.3), # 어파인 변환\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "transform_test  = A.Compose([\n",
    "    A.Resize(450, 650),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "img_dir = data_path+'images/'\n",
    "dataset_train = ImageDataSet(train, img_dir=img_dir, transform=transform_train)\n",
    "dataset_valid = ImageDataSet(valid, img_dir=img_dir, transform=transform_test)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "batch_size = 4\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=2\n",
    ")\n",
    "loader_valid = DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g,\n",
    "    num_workers=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.4.1 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006, weight_decay=0.0001)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# 스케쥴러 생성\n",
    "from torch.optim import lr_scheduler\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer=optimizer,\n",
    "    T_0=20,\n",
    "    T_mult=1,\n",
    "    eta_min=1e-6\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_iter_loss = []\n",
    "    for idx, (images, labels) in tqdm(enumerate(loader_train)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_iter_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 스케쥴러 학습률 갱신\n",
    "        scheduler.step()\n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - train loss : {sum(train_iter_loss)/len(loader_train):.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    valid_iter_loss = []\n",
    "    preds_list = []\n",
    "    true_onehot_list = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels) in tqdm(enumerate(loader_train)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_iter_loss.append(loss.item())\n",
    "            preds = torch.softmax(outputs.cpu(), dim=1).numpy()\n",
    "            true_onehot = torch.eye(4)[labels.cpu()].numpy()\n",
    "            preds_list.extend(preds)\n",
    "            true_onehot_list.extend(true_onehot)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - valid loss : {sum(valid_iter_loss)/len(loader_valid):.4f}')\n",
    "    print(f'Epoch [{epoch+1}/{epochs}] - valid roc auc : {roc_auc_score(true_onehot_list, preds_list):.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.4.2 예측\n",
    "* TTA(테스트 단계 데이터 증강)\n",
    "- 테스트 단계에서 활용하는 데이터 증강 기법\n",
    "- 진행 절차\n",
    "    - 테스트 데이터에 여러 변환 적용\n",
    "    - 변환된 테스트 데이터별로 타깃 확률값을 예측\n",
    "    - 타깃 예측 확률의 평구 구함"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 테스트 데이터 원본용 데이터셋 및 데이터 로더\n",
    "dataset_test = ImageDataSet(test, img_dir=data_path+'images/', transform=transform_test, is_test=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=2)\n",
    "\n",
    "# TTA용 데이터셋 및 데이터 로더\n",
    "dataset_TTA = ImageDataSet(test, img_dir=data_path+'images/', transform=transform_train, is_test=True)\n",
    "loader_TTA = DataLoader(dataset_TTA, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 예측\n",
    "model.eval()\n",
    "preds_test = np.zeros((len(test),4))\n",
    "with torch.no_grad():\n",
    "    for i, images in enumerate(loader_test):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds_part = torch.sortmax(outputs.cpu(), dim=1).squeeze().numpy()\n",
    "        preds_test[i*batch_size: (i+1)*batch_size] += preds_part"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 제출 샘플 복사\n",
    "submission_test = submission.copy()\n",
    "submission_test[['healthy', 'multiple_diseases', 'rust', 'scab']] = preds_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TTA 횟수\n",
    "num_TTA = 7\n",
    "\n",
    "preds_tta = np.zeros((len(test), 4))\n",
    "for i in range(num_TTA):\n",
    "    with torch.no_grad():\n",
    "        for i, images in enumerate(loader_TTA):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds_part = torch.softmax(outputs.cpu(), dim=1).squeeze().numpy()\n",
    "            preds_tta[i*batch_size:(i+1)*batch_size] += preds_part\n",
    "preds_tta /= num_TTA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission_tta = submission.copy()\n",
    "submission_tta[['healthy', 'multiple_diseases', 'rust', 'scab']] = preds_tta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 제출 파일 생성\n",
    "submission_test.to_csv(data_path+'submission_test.csv', index=False)\n",
    "submission_tta.to_csv(data_path+'submission_tta.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 레이블 스무딩\n",
    "* 과잉 확신한 예측값을 보정\n",
    "* $(1-\\alpha) * preds + \\frac{\\alpha}{K}$\n",
    "* $\\alpha$ : 레이블 스무딩 강도\n",
    "* K : 타깃값 개수\n",
    "* preds : 예측 활률값\n",
    "* ex> preds : (0,0,1,0), $\\alpha$ : 0.1 => (0.025, 0.025, 0.925, 0.025)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_label_smoothing(df, target, alpha, threshold):\n",
    "    df_target = df[target].copy()\n",
    "    k = len(target) # 타깃값 개수\n",
    "\n",
    "    for idx, row in df_target.itterows():\n",
    "        if (row > threshold).any():\n",
    "            row = (1-alpha)*row + alpha/k\n",
    "            df_target.iloc[idx] = row\n",
    "    return df_target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "threshold = 0.999\n",
    "submission_test_ls = submission_test.copy()\n",
    "submission_tta_ls = submission_tta.copy()\n",
    "\n",
    "target = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "submission_test_ls[target] = apply_label_smoothing(submission_test_ls, target, alpha, threshold)\n",
    "submission_tta_ls[target] = apply_label_smoothing(submission_tta_ls, target, alpha, threshold)\n",
    "\n",
    "submission_test_ls.to_csv('submission_test_ls.csv', index=False)\n",
    "submission_tta_ls.to_csv('submission_tta_ls.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
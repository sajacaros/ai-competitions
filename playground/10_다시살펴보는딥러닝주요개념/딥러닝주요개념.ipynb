{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 10 다시 살펴보는 딥러닝 주요 개념\n",
    "* 학습 순서\n",
    "    * 인공 신경망\n",
    "        * 퍼셉트론\n",
    "        * 신경망\n",
    "        * 활성화 함수(시그모이드, ReLU, Leaky ReLU)\n",
    "        * 경사 하강법\n",
    "        * 순전파와 역전파\n",
    "    * 합성곱 신경망(CNN)\n",
    "        * 합성곱 계층\n",
    "        * 패딩과 스트라이드\n",
    "        * 풀링\n",
    "        * 전결합\n",
    "        * 전체 구조\n",
    "    * 성능 향상을 위한 딥러닝 알고리즘\n",
    "        * 드롭아웃\n",
    "        * 배치 정규화(정규화, 스케일 조정, 이동)\n",
    "        * 옵티마이저(모멘텀, Adagrad, RMSProp, Adam)\n",
    "        * 전이 학습\n",
    "## 10.1 인공신경망\n",
    "* Artificial Neural Network\n",
    "* 인간의 생물학적 신경망과 유사한 구조를 갖는 인공 시스템\n",
    "* 뉴런\n",
    "    * 가장 기본적인 신경 세포\n",
    "    * 가지 돌기 -> 축삭 돌기 -> 임계치를 넘을시 -> 말단\n",
    "### 10.1.1 퍼셉트론\n",
    "* Perceptron\n",
    "* 뉴런의 원리를 본떠 만든 인공 구조\n",
    "### 10.1.2 신경망\n",
    "* 입력층 -> 은닉층(0~n개) -> 출력층\n",
    "### 10.1.3 활성화 함수\n",
    "* Activation Function\n",
    "* 대표 활성화 함수\n",
    "    * 시그모이드 함수(sigmoid function)\n",
    "    * ReLU 함수(rectified linear unit function)\n",
    "    * Leaky ReLU\n",
    "### 10.1.4 경사 하강법\n",
    "* 신경망 훈련의 목표는 최적의 파라미터를 찾는 것\n",
    "    * 최적 파라미터는 손실 함수가 최소값일 때의 파라미터를 가리킴\n",
    "* 손실 함수(loss function)\n",
    "    * 모델 성능을 측정하는 함수\n",
    "* 경사 하강법(gradient descent)\n",
    "    * 경사를 따라 내려가는 방법\n",
    "    * 지금 있는 위치에서 경사가 `가장 가파른 아래` 방향으로 `조금씩` 내려감\n",
    "* 확률적 경사 하강법(stochastic gradient descent, SGD)\n",
    "    * 전체 학습 데이터에서 개별 데이터를 무작위로 뽑아 경사 하강법을 수행\n",
    "* 미니배치 경사 하강법(minibatch gradient descent)\n",
    "    * 미니배치 단위로 무작위로 추출해 경사 하강법을 수행\n",
    "### 10.1.5 순전파와 역전파\n",
    "* 순전파(forward propagation)\n",
    "    * 입력값과 가중치를 활용해 출력값을 구하는 과정\n",
    "* 역전파(back propagation)\n",
    "    * 손실값을 통해 기울기를 구해 가중치를 갱신하는 과정\n",
    "## 10.2 합성곱 신경망(CNN)\n",
    "* convolutional neural network, CNN\n",
    "* ![cnn](../../images/cnn.png)\n",
    "### 10.2.1 합성곱 계층\n",
    "* convolutional layer\n",
    "* 합성곱(convolution)\n",
    "    * 2차원 데이터의 일정 영역 내 값들을 하나의 값으로 압축하는 연산\n",
    "    * 압축하기 위해 필터를 사용\n",
    "* 필터/커널(filter, kernel)\n",
    "    * 입력 데이터에서 특정한 특성을 필터링하는 역할\n",
    "    * 특정 특성을 부각시키거나 약화시키는 등의 변화를 줌\n",
    "* 특성 맵(feature map)\n",
    "    * 합성곱 연산으로 얻은 결과\n",
    "### 10.2.2 패딩과 스트라이드\n",
    "* 패딩(padding)\n",
    "    * 입력 데이터 주변을 특정 값으로 채우는 것\n",
    "* 스트라이드(stride)\n",
    "    * 이동하는 간격\n",
    "* 피쳐맵 크기 계산 방식\n",
    "    * 입력 데이터 크기 - N$~in~$\n",
    "    * 필터 크기 - K\n",
    "    * 패딩 크기 - P\n",
    "    * 스트라이드 크기 - S\n",
    "    * 출력 데이터 크기 - N$~out~$\n",
    "    * $N~out~ = \\frac{N~in~ + 2P - K}{S} + 1$\n",
    "### 10.2.3 풀링\n",
    "* 풀링(pooling)\n",
    "    * 특성 맵 크기를 줄여 이미지의 요약 정보를 추출하는 기능\n",
    "* 목적\n",
    "    * 특성 맵 크기를 줄여 연산 속도를 빠르게 하기 위함\n",
    "    * 이미지에서 물체의 위치가 바뀌어도 같은 물체로 인식하기 위해서\n",
    "* 풀링 방법\n",
    "    * 최대 풀링\n",
    "    * 평균 풀링\n",
    "* 풀링 크기와 스트라이드 크기를 같은 값으로 설정하는게 일반적\n",
    "* 피쳐맵 크기\n",
    "    * $N~out~ = \\frac{N~in~ + 2P - K}{S} + 1$\n",
    "    * K와 S가 같고 P가 0일시\n",
    "        * $N~out~ = \\frac{N~in~}{S}$\n",
    "### 10.2.4 전결합\n",
    "* 전결합(fully-connected) 계층\n",
    "    * 이전 계층의 모든 노드가 다음 계층의 노드 전부와 연결된 결합 계층\n",
    "    * 평탄화 작업 후 계층 마지막 부분에 구현\n",
    "    * 밀집 계층(dense layer)라고도 함\n",
    "* 평탄화(flatten)\n",
    "    * 2차원 데이터를 1차원 데이터로 바꾸는 작업\n",
    "* 전결합 계층을 여러개로 만드는 이유\n",
    "    * 분류를 효율적으로 하기 위해서\n",
    "### 10.2.5 합성곱 신경망 전체 구조\n",
    "* ![cnn](../../images/cnn.png)\n",
    "## 10.3 성능 향상을 위한 딥러닝 알고리즘\n",
    "### 10.3.1 드롭아웃\n",
    "* dropout\n",
    "* 과대적합을 방지하기 위해여 신경망 훈련 과정에서 무작위로 일부 뉴런을 제외하는 기법\n",
    "* 이터레이션마다 신경망의 뉴런 조합이 달라지므로 매번 다른 신경망을 훈련하는 형태가 됨\n",
    "* 앙상블 효과 발생\n",
    "* 훈련단계에서만 적용\n",
    "### 10.3.2 배치 정규화\n",
    "* batch normalization\n",
    "* 과대적합 방지와 훈련 속도 향상을 위한 기법\n",
    "* 내부 공변량 변화 현상을 해결하기 위한 기법\n",
    "* 내부 공변량 변화(internal covariate shift)\n",
    "    * 신경망 계층마다 입력 데이터 분포가 다른 현상\n",
    "    * 계층 간 데이터 분포의 편차를 줄이는 작업\n",
    "* 정규화한 데이터를 확대/축소(scale)하고 이동(shift) 변환까지 수행\n",
    "### 10.3.3 옵티마이저\n",
    "* 옵티마이저(optimaizer)\n",
    "    * 신경망의 최적 가중치를 찾아주는 알고리즘\n",
    "* SGD\n",
    "    * 확률적 경사 하강법\n",
    "* 모멘텀\n",
    "    * SDG에 물리학의 관성 개념을 추가한 옵티마이저\n",
    "* Adagrad\n",
    "    * 최적점에서 멀 때는 학습률을 크게 잡아 빠르게 수렴, 최적점에 가까워졌을 때는 학습률을 낮춰 최적점을 낮움\n",
    "    * 적응적 학습률(adaptive learning rate)\n",
    "* RMSProp\n",
    "    * 오래 지속하면 학습률이 0에 수렴하는 Adagrad의 단점을 보완\n",
    "    * 최근 기울기만 고려해 학습률을 낮춤\n",
    "* Adam\n",
    "    * 딥러닝 모델을 설계할 때 가장 많이 사용하는 옵티마이저\n",
    "    * 모멘텀과 RMSProp의 장점을 결합한 방법\n",
    "### 10.3.4 전이 학습\n",
    "* 전이 학습(transfer learning)\n",
    "    * 사전 훈련됨 모델(pretrained model)에 약간의 학습을 더해 활용하는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}